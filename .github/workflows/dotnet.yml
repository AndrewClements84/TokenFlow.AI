# .NET Build, Test & Publish Workflow
# ----------------------------------
# Builds, tests, and publishes TokenFlow packages.
# Includes performance regression tracking and artifact uploads.
# You can toggle benchmarks with the RUN_BENCHMARKS flag below.

name: .NET Build, Test & Publish

on:
  push:
    branches: [ master, main ]
    tags:
      - 'v*.*.*'
  pull_request:
    branches: [ master, main ]

env:
  RUN_BENCHMARKS: false   # ğŸ” Set to 'true' to enable performance benchmarks

jobs:
  build:
    name: ğŸ§ª Build, Test & Verify
    runs-on: ubuntu-latest

    steps:
      - name: âœ… Checkout repository
        uses: actions/checkout@v4

      - name: ğŸ§° Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '8.0.x'

      - name: ğŸ’¾ Cache NuGet packages
        uses: actions/cache@v4
        with:
          path: ~/.nuget/packages
          key: ${{ runner.os }}-nuget-${{ hashFiles('**/packages.lock.json') }}
          restore-keys: |
            ${{ runner.os }}-nuget-

      - name: ğŸ“¦ Restore dependencies
        run: dotnet restore TokenFlow.AI.sln

      - name: ğŸ—ï¸ Build all projects
        run: dotnet build TokenFlow.AI.sln --no-restore --configuration Release

      - name: ğŸ§ª Run all tests (including Tools.Tests)
        run: dotnet test TokenFlow.AI.sln --no-build --configuration Release --verbosity normal --collect:"XPlat Code Coverage"

      # -------------------------------------------------------------------
      # ğŸ§ª Performance Benchmark & Regression Tracking (optional)
      # -------------------------------------------------------------------
      - name: ğŸ§© Ensure benchmark-results directory exists
        if: ${{ env.RUN_BENCHMARKS == 'true' }}
        run: mkdir -p benchmark-results/results

      - name: ğŸ§­ Run Benchmarks
        if: ${{ env.RUN_BENCHMARKS == 'true' }}
        run: dotnet run -c Release --project src/TokenFlow.Tools.Benchmarks

      - name: ğŸ—‚ï¸ Prepare latest benchmark output (merge all JSONs)
        if: ${{ env.RUN_BENCHMARKS == 'true' }}
        run: |
          echo "ğŸ” Searching for BenchmarkDotNet report files..."
          pwd
          ls -R benchmark-results || true

          FILES=$(find benchmark-results -type f -name "*report*.json" | grep -v baseline.json || true)
          if [ -z "$FILES" ]; then
            echo "âŒ No benchmark report JSON files found!"
            echo "Directory contents for debugging:"
            find . -maxdepth 5 -type f
            exit 1
          fi

          echo "âœ… Found benchmark files:"
          echo "$FILES"

          echo "[" > benchmark-results/latest.json
          FIRST=true
          for FILE in $FILES; do
            if [ "$FIRST" = true ]; then
              FIRST=false
            else
              echo "," >> benchmark-results/latest.json
            fi
            cat "$FILE" >> benchmark-results/latest.json
          done
          echo "]" >> benchmark-results/latest.json

          echo "âœ… Combined benchmark results written to benchmark-results/latest.json"
          ls -lh benchmark-results/latest.json

      - name: ğŸ Setup Python for comparison script
        if: ${{ env.RUN_BENCHMARKS == 'true' }}
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: ğŸ” Compare Benchmark Results
        if: ${{ env.RUN_BENCHMARKS == 'true' }}
        run: |
          echo "Comparing benchmark-results/latest.json with baseline.json..."
          python3 .github/scripts/compare_benchmarks.py benchmark-results/baseline.json benchmark-results/latest.json
        continue-on-error: false

      - name: ğŸ“¤ Upload benchmark artifacts
        if: ${{ env.RUN_BENCHMARKS == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark-results/
          if-no-files-found: error
          retention-days: 7
      # -------------------------------------------------------------------

      - name: ğŸ§® Verify benchmarks compile
        run: dotnet run --project src/TokenFlow.Tools.Benchmarks/TokenFlow.Tools.Benchmarks.csproj -c Release -- --list
        continue-on-error: true

      - name: ğŸ“Š Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./tests/**/coverage.cobertura.xml
          flags: unittests
          fail_ci_if_error: true
          verbose: true

      - name: ğŸŸ© Mark build status
        if: success()
        run: echo "âœ… Build and tests passed!"

      - name: ğŸŸ¥ Mark build failed
        if: failure()
        run: echo "âŒ Build or tests failed."

  publish:
    name: ğŸš€ Publish TokenFlow Packages
    needs: build
    if: startsWith(github.ref, 'refs/tags/v')
    runs-on: ubuntu-latest

    steps:
      - name: âœ… Checkout repository
        uses: actions/checkout@v4

      - name: ğŸ§° Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '8.0.x'

      - name: ğŸ“¦ Restore dependencies
        run: dotnet restore TokenFlow.AI.sln

      - name: ğŸ—ï¸ Build in Release mode
        run: dotnet build TokenFlow.AI.sln --configuration Release --no-restore

      - name: ğŸ“¦ Pack TokenFlow.Core
        run: dotnet pack src/TokenFlow.Core/TokenFlow.Core.csproj --configuration Release --no-build -o ./artifacts

      - name: ğŸ“¦ Pack TokenFlow.AI
        run: dotnet pack src/TokenFlow.AI/TokenFlow.AI.csproj --configuration Release --no-build -o ./artifacts

      - name: ğŸš€ Publish to NuGet (only on tagged release)
        run: dotnet nuget push "./artifacts/*.nupkg" --api-key ${{ secrets.NUGET_API_KEY }} --source https://api.nuget.org/v3/index.json
